{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis\n",
    "\n",
    "VisualFeatures.CAPTION - Generate a human-readable sentence that describes the content of an image.\n",
    "\n",
    "VisualFeatures.READ -  Also known as Optical Character Recognition (OCR). Extract printed or handwritten text from images. \n",
    "Note: For extracting text from PDF, Office, and HTML documents and document images, use the Document Intelligence service with the Read model. \n",
    "\n",
    "VisualFeatures.DENSE_CAPTIONS - Dense Captions provides more details by generating one-sentence captions for up to 10 different regions in the image, including one for the whole image.\n",
    "\n",
    "VisualFeatures.TAGS - Extract content tags for thousands of recognizable objects, living beings, scenery, and actions that appear in images.\n",
    "\n",
    "VisualFeatures.OBJECTS - Object detection. This is similar to tagging, but focused on detecting physical objects in the image and returning their location.\n",
    "\n",
    "VisualFeatures.SMART_CROPS - Used to find a representative sub-region of the image for thumbnail generation, with priority given to include faces.\n",
    "\n",
    "VisualFeatures.PEOPLE - Detect people in the image and return their location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-vision-imageanalysis\n",
    "%pip install matplotlib\n",
    "%pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Function to display an image.\n",
    "def show_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # Convert the image from BGR to RGB for displaying with matplotlib,\n",
    "    # because OpenCV uses BGR by default and matplotlib expects RGB.\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image with matplotlib.\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis(\"off\")  # Turn off axis labels.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Function to display an image with bounding boxes based on the analysis result.\n",
    "def show_image_with_boxes(image_path, result, type):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "     # Define the font and size\n",
    "    font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", 40)  # You can adjust the font size as needed\n",
    "\n",
    "    if type == \"objects\":\n",
    "        if result.objects is not None:\n",
    "            for object in result.objects.list:\n",
    "                # Draw bounding box\n",
    "                box = object.bounding_box\n",
    "                draw.rectangle([(box['x'], box['y']), (box['x'] + box['w'], box['y'] + box['h'])], outline=\"red\", width=2)\n",
    "                draw.text((box['x'], box['y'] - 10), f\"{object.tags[0].name} ({object.tags[0].confidence:.2f})\", fill=\"red\", font=font)\n",
    "    elif type == \"people\":\n",
    "        if result.people is not None:\n",
    "            for person in result.people.list:\n",
    "                # Draw bounding box\n",
    "                box = person.bounding_box\n",
    "                draw.rectangle([(box['x'], box['y']), (box['x'] + box['w'], box['y'] + box['h'])], outline=\"red\", width=2)\n",
    "                draw.text((box['x'], box['y'] - 10), f\"Confidence {person.confidence:.2f}\", fill=\"red\", font=font)\n",
    "    elif type == \"text\":\n",
    "        if result.read is not None:\n",
    "            for line in result.read.blocks[0].lines:\n",
    "                for word in line.words:\n",
    "                    # Draw rectangle over the word\n",
    "                    bounding_box = word.bounding_polygon\n",
    "                    draw.polygon([(point['x'], point['y']) for point in bounding_box], outline=\"red\" , width=3)\n",
    "\n",
    "    # Save or display the image with bounding boxes\n",
    "    image.show()  # To display the image\n",
    "    # image.save(\"output_image_with_boxes.jpg\")  # To save the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "azure_computer_vision_endpoint = os.environ[\"AZURE_COMPUTER_VISION_ENDPOINT\"]\n",
    "azure_computer_vision_key = os.environ[\"AZURE_COMPUTER_VISION_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Image and Create Client\n",
    "\n",
    "https://learn.microsoft.com/en-us/python/api/overview/azure/ai-vision-imageanalysis-readme?view=azure-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "image = \"../data/images/analyze_image.jpg\"\n",
    "show_image(image)\n",
    "\n",
    "# Read the image file as bytes\n",
    "with open(image, \"rb\") as image_file:\n",
    "    image_data = image_file.read()\n",
    "\n",
    "# Create an Image Analysis client\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=azure_computer_vision_endpoint,\n",
    "    credential=AzureKeyCredential(azure_computer_vision_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption, Dense Captions, Tags\n",
    "\n",
    "Possible list of visual features: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.visualfeatures?view=azure-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature for the image. This will be a synchronously (blocking) call.\n",
    "result = client.analyze(\n",
    "    image_data=image_data,\n",
    "    visual_features=[VisualFeatures.CAPTION, VisualFeatures.DENSE_CAPTIONS, VisualFeatures.TAGS],\n",
    "    gender_neutral_caption=True,  # Optional (default is False)\n",
    ")\n",
    "\n",
    "print(\"Caption:\")\n",
    "if result.caption is not None:\n",
    "    print(f\"   '{result.caption.text}', Confidence {result.caption.confidence:.4f}\")\n",
    "\n",
    "print(\"Dense Captions:\")\n",
    "for item in result.dense_captions['values']:\n",
    "    print(f\"   '{item['text']}', Confidence {item['confidence']:.4f}\")\n",
    "\n",
    "print(\"Tags:\")\n",
    "for item in result.tags['values']:\n",
    "    print(f\"   '{item['name']}', Confidence {item['confidence']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature for the image. This will be a synchronously (blocking) call.\n",
    "result = client.analyze(\n",
    "    image_data=image_data,\n",
    "    visual_features=[VisualFeatures.OBJECTS],\n",
    "    gender_neutral_caption=True,  # Optional (default is False)\n",
    ")\n",
    "\n",
    " # Print Objects analysis results to the console\n",
    "print(\" Objects:\")\n",
    "if result.objects is not None:\n",
    "    for object in result.objects.list:\n",
    "        print(f\"   '{object.tags[0].name}', {object.bounding_box}, Confidence: {object.tags[0].confidence:.4f}\")\n",
    "print(f\" Image height: {result.metadata.height}\")\n",
    "print(f\" Image width: {result.metadata.width}\")\n",
    "\n",
    "show_image_with_boxes(image, result, \"objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Crops\n",
    "\n",
    "Thumbnails often need to have a certain aspect ratio, where aspect ratio is defined as the width in pixels divided by the height in pixels. For example, 1.0 for a square image, or 1.77 for a 16:9 widescreen image.\n",
    "\n",
    "You can optionally request one or more aspect ratios by setting the `smart_crops_aspect_ratios` argument in the call to `analyze`. Supported values are from 0.75 to 1.8 (inclusive).\n",
    "\n",
    "If you do not set this value, the service will return one result with an aspect ratio it sees fit between 0.5 and 2.0 (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Smart Cropping analysis on an image stream. This will be a synchronously (blocking) call.\n",
    "result = client.analyze(\n",
    "    image_data=image_data,\n",
    "    visual_features=[VisualFeatures.SMART_CROPS],\n",
    "    smart_crops_aspect_ratios=[1.0],  # Optional. Specify one more desired aspect ratios    \n",
    ")\n",
    "\n",
    "if result.smart_crops is not None:\n",
    "    for smart_crop in result.smart_crops.list:\n",
    "        print(f\"Aspect ratio {smart_crop.aspect_ratio}: Smart crop {smart_crop.bounding_box}\")\n",
    "print(f\" Image height: {result.metadata.height}\")\n",
    "print(f\" Image width: {result.metadata.width}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Cropped Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the original image\n",
    "original_image = Image.open(image)\n",
    "\n",
    "# Define the bounding box (x, y, w, h)\n",
    "# you need to adjust the bounding box based on the smart crop result\n",
    "bounding_box = {'x': 957, 'y': 0, 'w': 2270, 'h': 2271}\n",
    "\n",
    "# Crop the image using the bounding box\n",
    "cropped_image = original_image.crop((bounding_box['x'], bounding_box['y'], \n",
    "                                     bounding_box['x'] + bounding_box['w'], \n",
    "                                     bounding_box['y'] + bounding_box['h']))\n",
    "\n",
    "# Display the cropped image\n",
    "cropped_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find people in an image stream. This will be a synchronously (blocking) call.\n",
    "result = client.analyze(\n",
    "    image_data=image_data,\n",
    "    visual_features=[VisualFeatures.PEOPLE]\n",
    ")\n",
    "\n",
    "# Print People analysis results to the console\n",
    "print(\" People:\")\n",
    "if result.people is not None:\n",
    "    for person in result.people.list:\n",
    "        print(f\"   {person.bounding_box}, Confidence {person.confidence:.4f}\")\n",
    "print(f\" Image height: {result.metadata.height}\")\n",
    "print(f\" Image width: {result.metadata.width}\")\n",
    "\n",
    "show_image_with_boxes(image, result, \"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"../data/images/ocr.jpg\"\n",
    "show_image(image)\n",
    "\n",
    "# Read the image file as bytes\n",
    "with open(image, \"rb\") as image_file:\n",
    "    image_data = image_file.read()\n",
    "\n",
    "\n",
    "# Extract text (OCR) from an image stream. This will be a synchronously (blocking) call.\n",
    "result = client.analyze(\n",
    "    image_data=image_data,\n",
    "    visual_features=[VisualFeatures.READ]\n",
    ")\n",
    "\n",
    "# Print text (OCR) analysis results to the console\n",
    "print(\" Read:\")\n",
    "if result.read is not None:\n",
    "    for line in result.read.blocks[0].lines:\n",
    "        print(f\"   Line: '{line.text}', Bounding box {line.bounding_polygon}\")\n",
    "        for word in line.words:\n",
    "            print(f\"     Word: '{word.text}', Bounding polygon {word.bounding_polygon}, Confidence {word.confidence:.4f}\")\n",
    "\n",
    "print(f\" Image height: {result.metadata.height}\")\n",
    "print(f\" Image width: {result.metadata.width}\")\n",
    "\n",
    "show_image_with_boxes(image, result, \"text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
