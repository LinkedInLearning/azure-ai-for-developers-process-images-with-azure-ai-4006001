{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Images using Azure OpenAI\n",
    "\n",
    "Pre-requisites:\n",
    "1. Create Azure OpenAI resource\n",
    "2. Deploy gpt-4 and above model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Prepare the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a Path object for the image file\n",
    "image_path = Path(\"images/generated_image.jpg\")\n",
    "\n",
    "# Using a context manager to open the file with Path.open()\n",
    "with image_path.open(\"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Prepare the image content in the required format for the Azure OpenAI service\n",
    "content_images = [\n",
    "    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "    for base64_image in [base64_image]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "# Create the Vision client\n",
    "vision_client = AsyncAzureOpenAI(\n",
    "    api_key=azure_openai_key, \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "vision_deployment_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The image depicts a quiet residential street during early dawn. A cat is seen walking in the middle of the road, facing away from the camera and heading further down the street. The street is wet, suggesting it may have rained recently, and there is a slight mist in the air contributing to the serene and calm atmosphere.\n",
      "\n",
      "On either side of the street, houses with pitched roofs and garages are lined up neatly. The sky has a light grayish-blue tint, indicating the start of a new day. Streetlights are on, casting a soft, warm light over the scene. Various objects such as trash bins and a parked car are visible in front of the houses, adding to the suburban setting. There is also a noticeable gradient in the road's surface, with a darker patch in the center, possibly due to water pooling or various tire tracks. The overall mood of the image is tranquil and peaceful.\n"
     ]
    }
   ],
   "source": [
    "# Define the user prompt for the image description\n",
    "user_prompt = \"Describe this image in detail.\"\n",
    "\n",
    "# Send a request to the Azure OpenAI service to analyze the image and generate a description\n",
    "response = await vision_client.chat.completions.create(\n",
    "    model=vision_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "                *content_images,  # Include the image content in the request\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000,  # Set the maximum number of tokens for the response\n",
    ")\n",
    "\n",
    "# Print the generated description of the image\n",
    "print(\"Response: \" + response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting results similar to Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1. \"Morning stroll before the neighborhood wakes up.\"\n",
      "2. \"This kitty is off on an early morning adventure.\"\n",
      "3. \"Exploring the quiet streets all alone.\"\n",
      "4. \"Curiosity leads the way.\"\n",
      "5. \"A cat's journey through an empty road.\"\n",
      "6. \"Early bird catches the worm, or in this case, cat!\"\n",
      "7. \"When the world belongs to just you and the sunrise.\"\n",
      "8. \"Paws on the pavement, eyes on the road.\"\n",
      "9. \"A peaceful start to the day with a feline friend.\"\n",
      "10. \"Walking towards a new day.\"\n"
     ]
    }
   ],
   "source": [
    "# Define the user prompt for the image description\n",
    "user_prompt = \"Provide me 10 captions for this image.\"\n",
    "\n",
    "# Send a request to the Azure OpenAI service to analyze the image and generate a description\n",
    "response = await vision_client.chat.completions.create(\n",
    "    model=vision_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "                *content_images,  # Include the image content in the request\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000,  # Set the maximum number of tokens for the response\n",
    ")\n",
    "\n",
    "# Print the generated description of the image\n",
    "print(\"Response: \" + response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ```json\n",
      "{\n",
      "    \"description\": \"A cat walks down a residential street lined with houses and cars during early morning.\",\n",
      "    \"category\": \"cat\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Define the user prompt for the image description\n",
    "user_prompt = \"\"\"Analyze this image.\n",
    "    Provide response in sample JSON format.\n",
    "    {\n",
    "        \"description\": \"Describe the image in less than 50 words\",\n",
    "        \"category\": cat, dog, or mouse\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Send a request to the Azure OpenAI service to analyze the image and generate a description\n",
    "response = await vision_client.chat.completions.create(\n",
    "    model=vision_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "                *content_images,  # Include the image content in the request\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000,  # Set the maximum number of tokens for the response\n",
    ")\n",
    "\n",
    "# Print the generated description of the image\n",
    "print(\"Response: \" + response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
